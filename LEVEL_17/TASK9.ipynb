{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ccc4dfe-8a43-490a-b093-533e0c1260d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------+----+---------------+\n|match_id|inning|ball_no|runs|cumulative_runs|\n+--------+------+-------+----+---------------+\n|       1|     1|      1|   1|              1|\n|       1|     1|      2|   0|              1|\n|       1|     1|      3|   4|              5|\n|       1|     1|      4|   2|              7|\n|       1|     2|      1|   6|              6|\n|       1|     2|      2|   1|              7|\n+--------+------+-------+----+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, sum as _sum\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CumulativeRuns\").getOrCreate()\n",
    "\n",
    "data = [\n",
    "    (1, 1, 1, 1),\n",
    "    (1, 1, 2, 0),\n",
    "    (1, 1, 3, 4),\n",
    "    (1, 1, 4, 2),\n",
    "    (1, 2, 1, 6),\n",
    "    (1, 2, 2, 1)\n",
    "]\n",
    "\n",
    "columns = [\"match_id\", \"inning\", \"ball_no\", \"runs\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "windowSpec = Window.partitionBy(\"match_id\", \"inning\").orderBy(\"ball_no\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df_with_cumulative = df.withColumn(\"cumulative_runs\", _sum(\"runs\").over(windowSpec))\n",
    "\n",
    "df_with_cumulative.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "505bc359-9a67-4582-a130-13f141c33929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------+------+-------+------+--------+------+-----------+\n|commentary                                          |bowler|batsman|runs  |match_id|inning|ball_no    |\n+----------------------------------------------------+------+-------+------+--------+------+-----------+\n|Bumrah to Dhawan, FOUR! Cracking shot through covers|Bumrah|Dhawan |FOUR  |1       |1     |8589934592 |\n|Chahal to Raina, 1 run, nudged to midwicket         |Chahal|Raina  |1 run |1       |1     |25769803776|\n|Narine to Kohli, no run, defended solidly           |Narine|Kohli  |no run|1       |1     |42949672960|\n|Rabada to Rohit, SIX! Smashed over long-on          |Rabada|Rohit  |SIX   |1       |1     |60129542144|\n+----------------------------------------------------+------+-------+------+--------+------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "data = [\n",
    "    (\"Bumrah to Dhawan, FOUR! Cracking shot through covers\", \"Bumrah\", \"Dhawan\", \"FOUR\"),\n",
    "    (\"Chahal to Raina, 1 run, nudged to midwicket\", \"Chahal\", \"Raina\", \"1 run\"),\n",
    "    (\"Narine to Kohli, no run, defended solidly\", \"Narine\", \"Kohli\", \"no run\"),\n",
    "    (\"Rabada to Rohit, SIX! Smashed over long-on\", \"Rabada\", \"Rohit\", \"SIX\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"commentary\", StringType(), True),\n",
    "    StructField(\"bowler\", StringType(), True),\n",
    "    StructField(\"batsman\", StringType(), True),\n",
    "    StructField(\"runs\", StringType(), True)\n",
    "])\n",
    "\n",
    "commentary_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "commentary_df = commentary_df.withColumn(\"match_id\", lit(1)) \\\n",
    "                             .withColumn(\"inning\", lit(1)) \\\n",
    "                             .withColumn(\"ball_no\", monotonically_increasing_id())\n",
    "\n",
    "commentary_df.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0176dd9-cd19-4540-bc56-31cc91858a86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "commentary_df = commentary_df.withColumn(\n",
    "    \"runs\",\n",
    "    when(col(\"runs\").like(\"%FOUR%\"), 4)\n",
    "    .when(col(\"runs\").like(\"%SIX%\"), 6)\n",
    "    .when(col(\"runs\").like(\"%1 run%\"), 1)\n",
    "    .when(col(\"runs\").like(\"%2 run%\"), 2)\n",
    "    .when(col(\"runs\").like(\"%3 run%\"), 3)\n",
    "    .when(col(\"runs\").like(\"%no run%\"), 0)\n",
    "    .otherwise(0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a53e193-1a2b-4650-bcb6-48e79dc74c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+-------+----+---------------+\n|match_id|inning|    ball_no|batsman|runs|cumulative_runs|\n+--------+------+-----------+-------+----+---------------+\n|       1|     1| 8589934592| Dhawan|   4|              4|\n|       1|     1|25769803776|  Raina|   1|              5|\n|       1|     1|42949672960|  Kohli|   0|              5|\n|       1|     1|60129542144|  Rohit|   6|             11|\n+--------+------+-----------+-------+----+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "windowSpec = Window.partitionBy(\"match_id\", \"inning\").orderBy(\"ball_no\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "ball_df_with_cumulative = commentary_df.withColumn(\"cumulative_runs\", _sum(\"runs\").over(windowSpec))\n",
    "\n",
    "ball_df_with_cumulative.select(\"match_id\", \"inning\", \"ball_no\", \"batsman\", \"runs\", \"cumulative_runs\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc870a1d-bf8c-486d-8c32-9150fa256c45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LEVEL 17 T9",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}